<!DOCTYPE html>
<html>
  <head>
    <title>drake for Workflow Management in R</title>
    <meta charset="utf-8">
    <meta name="author" content="Amanda Dobbyn" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/shinobi.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <code>drake</code> for Workflow Management in R
### Amanda Dobbyn

---





###  Warning: this presentation contains less rap than you might have expected. 

I won't blame you if you want to make a quick getaway.

.center[![grandpasimpson](https://media.giphy.com/media/11gC4odpiRKuha/giphy.gif)]

---

class: inverse

### Quick about me

Day job: ultimate frisbee player.

For fun: Data Scientist at Earlybird Software, former co-organizer of [R-Ladies Chicago](https://rladieschicago.org/).

GitHub: https://github.com/aedobbyn

Website: https://dobb.ae

Twitter: @dobbleobble

---

### `drake`'s main idea

[`drake`](https://github.com/ropensci/drake) is workflow manager for your R code.

*What that means*: 

In an R pipeline, when changes occur that make the most recent results **out-of-date**, `drake` rebuilds *only* the parts of the pipeline that need to be rebuilt.

&lt;p align="center"&gt;
&lt;img src="https://media.giphy.com/media/JFawGLFMCJNDi/giphy.gif" alt="ilovechanges"&gt;
&lt;/p&gt;

Created and maintained by [Will](https://twitter.com/wmlandau) [Landau](https://github.com/wlandau) and friends.

---

## What's the deal with the name?

--

### **d**ataframes in **R** for M**ake**

--

.pull-left[

What's Make?

[GNU Make](https://www.gnu.org/software/make/) is a tool that uses file called a Makefile to specify **dependencies** and how targets should be rebuilt when they're out of date. 

`drake` takes that idea and implements it in a way that's more native to how we work in R.

]

Example of a Makefile:

.pull-right[![example_makefile](./img/example_makefile_r.jpg)]

---

### Nice features of `drake`
1. See how your pipeline fits together, in a tidy **dataframe**

--

2. Visualize the **dependency graph** of your code

--

3. Great for iteration and **reproducibility**; you know exactly how these results were generated 

--

4. High-performance computing advantages

--

5. It's all in R so no writing config files! ðŸŽ‰

---

### Better workflows

.pull-left[

Does your directory look like this?

`01_import.R`

`02_clean.R`

`03_deep_clean.R`

`04_join.R`

`05_analyze.R`

`06_analyze_more.R`

`07_report.Rmd`
]

.pull-right[

- Depends on correctly `source`ing the script before it. Need:
  - Up-to-date file names
  - Same input data
  - Everything working correctly and in the right order


- If something breaks in `04_join.R` 
  - Can you be sure about where it broke?
  - Do you need to re-run the entire pipeline again? 
]



---

### `drake` : `knitr`

(Analogy stolen from Will's point in his [interview on the R podcast](https://www.youtube.com/watch?v=eJQ29CLyDCs&amp;feature=youtu.be&amp;t=1533).)

![tiny_hats](./img/tiny_hats.jpg)




1) `knitr` can **cache** chunks if they've already been run, and nothing in them has changed.

2) A chunk successfully knitting **depends** on the previous chunk knitting and on any chunk that you specify a [`depedson`](https://twitter.com/drob/status/738786604731490304?lang=en) for.

3) It lives in a single file, making your analysis reproducible and **compact**. 




???
With knitr, you expect to be able to rerun someone's report from a single file.

---

## A few pieces of vocab


**Targets** are the objects and files that drake generates, and

--

**commands** are the pieces of R code that produce them.

--

**Plans** wrap up the relationship between targets and commands into a workflow representation: a dataframe.

???

one column for targets, and one column for their corresponding commands.

---

## More on plans

Plans are like that top-level script that runs the whole thing, like


```r
source("01_import.R")
source("02_clean.R")
...
source("06_analyze_more.R")

final &lt;- do_more_things(object_in_env)

write_out_my_results(final)
```


*But*, the plan **knows about the dependencies** in your code.


---

## How to `drake`

1. Store **prework** (loading packages and user-defined functions) in a file 
2. Store a `drake` **plan** in another file
3. Create the plan and **run** it

--


```r
source("/path/to/my/prework.R")

plan &lt;- 
  drake_plan(
    cleaned_data = clean_my(raw_data),
    results = analyze_my(cleaned_data),
    report = 
      report_out_my(results,
          file_out = "/path/to/my/report.md")
  )

make(plan)
```

---

## What `drake does`


```r
plan &lt;- 
  drake_plan(
    cleaned_data = clean_my(raw_data),
    results = analyze_my(cleaned_data),
    report = 
      report_out_my(results,
          file_out = "/path/to/my/report.md")
  )

make(plan)
```

**First run** of `make(plan)`:

`drake` runs the plan from scratch

&lt;br&gt;

**Thereafter**

`drake` will only rebuild targets that are out of date, and everything downstream of them



---

### What makes a target become out of date?

1) A trigger is activated (more on these later)

&lt;br&gt;

2) Some part of the code used to generate that target or one of its upstream targets has changed


```r
plan &lt;- 
  drake_plan(
    cleaned_data = clean_my(raw_data),
    results = analyze_my(cleaned_data),
    report = report_out_my(results)
  )
```


`drake` knows that`results` depends on `cleaned_data`

because `results` is generated by the `analyze_my(cleaned_data)` command


---

## Where is all this info stored?

**targets**

In a hidden `.drake` cache. `drake` uses hashes to know when things change. [More on storage.](https://ropensci.github.io/drake/articles/storage.html)

`clean()` cleans that cache.

&lt;br&gt;

**dependencies**

`drake` stores a dependency graph (`igraph` object) of the plan along with a bunch of other things in a `config`.

You can access all of it with `drake_config()`.

---

## All about Functions

`drake` is all built around *functions* rather than scripts.

- A plan works by using functions to create targets

&lt;br&gt;

- This allows `drake` to infer dependencies between
  - objects and functions
  - functions and other functions
  
&lt;br&gt;

- Running `drake_plan` creates a dataframe relating each target to the command used to generate it

---

## All about Functions




```r
bad_plan &lt;- 
  drake_plan(
    first_target = source("import.R"),
    second_target = source("clean.R")
  )
```

--

Sourcing files breaks the dependency structure that makes `drake` useful. 

--

Instead, 


```r
source("all_my_funs.R")

good_plan &lt;- 
  drake_plan(
    first_target = do_stuff(my_data),
    second_target = do_more_stuff(first_target)
  )
```

Now `drake` knows`first_target` needs to be built before work on `second_target` can begin.

---

### Other things `drake` can do that we won't get into

- [Generate ~ big plans ~](https://ropensci.github.io/drake/articles/best-practices.html#generating-workflow-plan-data-frames)

.pull_right[![big_plans](https://media.giphy.com/media/DCslYLxUUgKX2ms0iw/giphy.gif)]

for analyses that require lots of different permutations of a similar analysis

(version 7.0.0 has new experimental syntax that makes it easier to create big plans)

- Support for [debugging and testing ](https://ropenscilabs.github.io/drake-manual/debug.html) plans

- Compatibility with [high performance computing](https://ropenscilabs.github.io/drake-manual/hpc.html) backends

---

### Moar Resources

- [`drake` user manual](https://ropenscilabs.github.io/drake-manual/index.html)
- [debugging drake](https://ropensci.github.io/drake/articles/debug.html)
- [Kirill MÃ¼ller's cheat sheet](https://github.com/krlmlr/drake-sib-zurich/blob/master/cheat-sheet.pdf)
- [Sina RÃ¼eger](https://sinarueeger.github.io/2018/10/09/workflow/) and [Christine Stawitz](https://github.com/cstawitz/RLadies_Sea_drake)'s `drake` presentations

---

### Our plan

I'll illustrate a way you might want to use `drake` with something that's close to home for us.

Remember the [crazy blue light](https://twitter.com/NYCFireWire/status/1078478369036165121) from late December?

The Twitter account that let us know that this wasn't in fact aliens is [NYCFireWire](https://twitter.com/NYCFireWire).

Normally they just tweet out fires and their locations in a more or less predictable pattern, i.e.

`&lt;borough&gt; &lt;** some numbers **&gt; &lt;address&gt; &lt;description of fire&gt;`

What if we were constructing an analysis of these tweets and wanted to make sure our pipeline worked end-to-end, but didn't want to unnecessarily re-run outdated parts of it unless we needed to?

---

### The Pipeline

1. Pull in tweets, either the first big batch or any new ones that show up
2. Extract addresses from the tweets (ðŸŽ¶ regex time ðŸŽ¶)
3. Send addresses to the Google Maps API to grab their latitudes and longitudes
4. Profit

All functions stored in [`didnt_start_it.R`](https://github.com/aedobbyn/nyc-fires/blob/master/R/didnt_start_it.R).


```r
source(here::here("R", "didnt_start_it.R"))
```

Caveat is that this analysis relies on the [rtweet](https://github.com/mkearney/rtweet) and [ggmap](https://github.com/dkahle/ggmap) packages.

To be able to run it in full you'll need a [Twitter API access token](https://rtweet.info/articles/auth.html) and [Google Maps Geocoding API key](https://developers.google.com/maps/documentation/geocoding/intro#Geocoding).

---

### Grabbing tweets

- `get_seed_tweets` grabs a batch of tweets *or* reads in seed tweets from a file if the file exists
- `get_more_tweets` checks if there are new tweets and, if so, pulls in the right number of them
- `get_tweets` runs `get_seed_tweets` if given a null `tbl` argument, otherwise runs `get_more_tweets`




```r
get_tweets()
```

---

### Grabbing tweets

The most recent tweets from NYCFireWire:


```r
get_tweets() %&gt;% 
  select(text) %&gt;% 
  print(n = nrow(.))
```


---

### Grabbing tweets

Starting with an old tweet ID, we can first grab a 10 tweets older than `old_tweet_id` and then, running the same function again, add 5 of the most recent tweets to our dataframe.


```r
old_tweet_id &lt;- "1084619203167031297" # Random tweet from a while ago so what we can set a max id in the past

seed_tweets &lt;- 
  get_tweets(
    n_tweets_seed = 10,
    max_id = old_tweet_id
  )

(full_tweets &lt;- 
  get_tweets(seed_tweets, n_tweets_reup = 5,
             output_path = here("data", "raw", "fires.csv")))
```

---

### Getting addresses




```r
get_tweets() %&gt;% 
  pull_addresses() %&gt;% 
  select(street, borough, address, text)
```

---

### Getting lat and long

Then we can use the [`geocode`](https://www.rdocumentation.org/packages/ggmap/versions/2.6.1/topics/geocode) function (along with a [Google Maps](https://cloud.google.com/maps-platform/) API key) from the `ggmap` package to attach the latitude and longitude to each address, if Google can find it. (Otherwise we're returned `NA`s.)




```r
get_tweets(n_tweets_seed = 5) %&gt;% 
  pull_addresses() %&gt;% 
  get_lat_long()
```


That's our main pipeline. We can do a few bits of analysis of the data after that.

---

### Late pipeline

Later in the pipeline we can join 


```r
nyc &lt;-
  ggplot2::map_data("state", region = "new york") %&gt;%
  truncate_lat_long(digits = 1) %&gt;%
  as_tibble()
```


or `count_fires`, summing up the total number of fires per `lat`-`long` combo.


```r
get_tweets(n_tweets_seed = 5) %&gt;% 
  pull_addresses() %&gt;% 
  get_lat_long() %&gt;% 
  count_fires()
```


---

### Quick benchmark

Even though it didn't take too long to pull 5 tweets from Twitter and send 5 addresses to Google Maps, we wouldn't want to re-run this part of the pipeline every time we changed an analysis step.



```r
b_mark &lt;-
  bench::mark({
    get_tweets(n_tweets_seed = 1) %&gt;% 
    pull_addresses() %&gt;% 
    get_lat_long()
})

b_mark %&gt;% 
  select(median)
```




---

### Live coding time!

The `rtweet` package also supports posting tweets, so we can test out whether our trigger successfully pulls in new tweet by posting ourselves with a

ðŸ”¥ **[burner account!](https://twitter.com/didntstartit)** ðŸ”¥
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>

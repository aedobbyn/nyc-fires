---
title: "pres"
author: "Amanda Dobbyn"
date: "1/20/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source(here::here("didnt_start_it.R"))
```

### Quick about me

### `drake`'s main idea

In a pipeline, when changes occur that make the most recent results out-of-date, rebuild *only* the parts of the pipeline that need to be rebuilt.

### Why the name `drake`?

[GNU Make](https://www.gnu.org/software/make/) for R, built around dataframes.

GNU Make uses a file called a Makefile to specify dependencies and how targets should be reubilt when they're out of date. 

### Two pieces of vocab

*targets* and *commands*
From `?drake_config`: "**Targets** are the objects and files that drake generates, and **commands** are the pieces of R code that produce them."

### Other nice features of `drake`
1. See how your pipeline fits together, in a tidy dataframe
2. Visualize dependencies in your code in graph format
3. Some high-performance computing advantages
4. Great for reproducibility

### How to `drake`:
1. Store prework (loading packages and user-defined functions) in a file
2. Store a `drake` plan in another file
3. Create the plan and run it

```{r, eval=FALSE}
source("/path/to/my/prework.R")

# Custom functions defined in prework are:
# clean_my(), analyze_my(), and report_out_my()

plan <- drake_plan(
  cleaned_data = clean_my(raw_data),
  results = analyze_my(cleaned_data),
  report = report_out_my(results,
                         file_out = "/path/to/my/report.md")
)

make(plan)
```

The first run of `make(plan)` will run the plan from scratch. 

After that, `drake` will only re-run the parts of the plan that are out of date, and all those parts' dependencies.


### What makes a target become out of date?

1. Some part of the code used to generate that target or one of its upstream targets has changed
2. A trigger is activated (more on that later)

### Big Idea #2

`drake` is all built around *functions*.
- A plan works by using functions to create targets. 
- Running `drake_plan` creates a dataframe relating each target to the function used to generate it.

How does `drake` store targets that have been successfully built?
In a hidden `.drake` directory. In the `data` subdirectory, these are stored as `rds` objects.

How does `drake` keep track of dependencies?
With a `config` which you can access.


```{r}

bad_plan <- 
  drake_plan(
    first_target = source("import.R"),
    second_target = 
  )

```


### Our plan

Remember the [crazy blue light](https://twitter.com/NYCFireWire/status/1078478369036165121) from late December?

The Twitter account that let us know that this wasn't in fact aliens is [@NYCFireWire](https://twitter.com/NYCFireWire).

Normally they just tweet out fires and their locations in a more or less predictable pattern, e.g.

https://twitter.com/NYCFireWire/status/1087888350277705728


What if we were constructing an analysis of these tweets and wanted to make sure our pipeline worked end-to-end, but didn't want to unnecessarily re-run outdated parts of it unless we needed to?


### The Pipeline

1. Pull in tweets, either the first big batch or any new ones that show up
2. Extract addresses from the tweets
3. Send these to the Google Maps API to grab the addresses latitudes and longitudes
4. Analyze

### Grabbing tweets

```{r}
get_seed_tweets

get_more_tweets

get_tweets
```


### Write our plan

```{r}
fire_path <- here("data", "raw", "fires.csv")

plan <-
  drake_plan(
    seed_fires = get_tweets( # Grab some seed fires
      n_tweets_seed = 2
    ), 
    fires = target(
      command = get_tweets(
        tbl = seed_fires,
        n_tweets_reup = 3
      ),
      trigger = trigger(
        condition = TRUE # Always look for new tweets
      )
    ),
    addresses = pull_addresses(fires), # Extract addresses from tweets
    lat_long = get_lat_long(addresses), # Send to Google for lat-longs
    dat = join_on_city_data(lat_long, nyc), # Join on the nyc coords
    fire_sums = count_fires(dat), # Sum up n fires per lat-long combo

    time_graph = graph_fire_times(dat),
    plot = plot_fires(dat, nyc)
  )
```

### Run our plan

```{r}
plan

make(plan, verbose = 4)
```


```{r, eval=FALSE}
config <- drake_config(plan)
config$plan
config$prework
config$targets
config$cache_path
config$graph

vis_drake_graph()

loadd(dat)
dat

clean(fires)

outdated(config)

vis_drake_graph()

make(plan)
```

If our `seed_fires` had been 3000 instead of 3:

```{r}
full_dat <- read_csv(here("data", "derived", "dat.csv"))

graph_fire_times(full_dat)
```


### Testing our reupping with a `r emo::ji("fire")` burner account `r emo::ji("fire")`

The `rtweet` package also supports posting tweets, so we can test out whether our trigger successfully pulls in new tweet by posting ourselves.

```{r}
burner_path <- "data/derived/burn.csv"

burner_plan <-
  drake_plan(
    seed_burn = get_tweets(
      user = burner_handle,
      input_path = burner_path # Reads in file from burn_path if file exists, otherwise pulls in seed tweets
    ),
    
    full_burn = target(
      command = get_tweets(
        tbl = seed_burn,
        user = burner_handle,
        output_path = burner_path
      ),
      trigger = trigger(
        condition = TRUE # Always look for new tweets
      )
    )
  )

burner_plan
```

### Test out the burner

```{r, eval=FALSE}
burner_config <- drake_config(burner_plan)
make(burner_plan)
loadd(seed_burn)
loadd(full_burn)
expect_equal(seed_burn, full_burn)

outdated(burner_config)

# Tweet here
post_tweet(status = 
             digest::digest(sample(100, 1)), 
           token = firewire_token)

make(burner_plan)

loadd(seed_burn)
loadd(full_burn)
burn <- read_csv(here("data", "derived", "burn.csv"))
expect_gt(nrow(full_burn), nrow(seed_burn))
expect_equal(nrow(full_burn), nrow(burn))

clean()

make(burner_plan)
loadd(seed_burn)
loadd(full_burn)
expect_equal(nrow(seed_burn), nrow(full_burn))

```



Resources:
[`drake` user manual`](https://ropenscilabs.github.io/drake-manual/index.html)
[cheat sheet](https://github.com/krlmlr/drake-sib-zurich/blob/master/cheat-sheet.pdf)
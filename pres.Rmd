---
title: "pres"
author: "Amanda Dobbyn"
date: "1/20/2019"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source(here::here("didnt_start_it.R"))
```

### Quick about me

Day job: ultimate firsbee player.

For fun: Data Scientist at Earlybird Software, former co-organzier of R-Ladies Chicago.

### `drake`'s main idea

In an R pipeline, when changes occur that make the most recent results out-of-date, rebuild *only* the parts of the pipeline that need to be rebuilt.

### What is `drake`?

A workflow manager for your R code .

[GNU Make](https://www.gnu.org/software/make/) for R, built around dataframes.

GNU Make uses workflow outline file called a Makefile to specify dependencies and how targets should be reubilt when they're out of date. 

Created and maintained by [Will](https://twitter.com/wmlandau) [Landau](https://github.com/wlandau) and friends.

### Two pieces of vocab

*targets* and *commands*

From `?drake_config`: "**Targets** are the objects and files that drake generates, and **commands** are the pieces of R code that produce them."

### Analogy to `knitr`

(Stolen from Will's analogy in his [interview on the R podcast](https://www.youtube.com/watch?v=eJQ29CLyDCs&feature=youtu.be&t=1533).)

`drake` is sort of like `knitr` in the sense that 
1) It makes your analysis reproducible and compact. You expect to be able to rerun someone's report from a single file.

2) In `knitr`, chunks can be cached if they've already been run, so they don't need to be re-run unless something in them changes.

3) When an `Rmd` is knit, a chunk successfully knitting depends on the previous chunk knitting and on any chunk that you specify a `depedson` for.


### Other nice features of `drake`
1. See how your pipeline fits together, in a tidy dataframe
2. Visualize dependencies in your code in graph format
3. Some high-performance computing advantages
4. Great for iteration and reproducibility; you know exactly how these results were generated
5. It's all in R -- no writing config files!

### How to `drake`:
1. Store prework (loading packages and user-defined functions) in a file 
2. Store a `drake` plan in another file
3. Create the plan and run it

```{r, eval=FALSE}
source("/path/to/my/prework.R")

## Custom functions defined in prework are:
# clean_my(), analyze_my(), and report_out_my()

plan <- 
  drake_plan(
  cleaned_data = clean_my(raw_data),
  results = analyze_my(cleaned_data),
  report = report_out_my(results,
                         file_out = "/path/to/my/report.md")
)

make(plan)
```

The first run of `make(plan)` will run the plan from scratch. 

After that, `drake` will only re-run the parts of the plan that are out of date and everything downstream of that.

`drake` knows that, for example, `results` depends on `cleaned_data` because `cleaned_data` is part of the `analyze_my(cleaned_data)` command used to generate `results`.


### What makes a target become out of date?

1. Some part of the code used to generate that target or one of its upstream targets has changed
2. A trigger is activated (more on that later)


### Where is all this stored?

How does `drake` store info/data about targets?
In a hidden `.drake` cache when you run `make()`. In the `.drake/data` subdirectory, targets are stored as hashed `rds` objects. [More on storage.](https://ropensci.github.io/drake/articles/storage.html)

`clean()` cleans that cache.

How does `drake` keep track of dependencies? 
`drake` stores dependency graph along with a bunch of other things in `config`, which you can access with `drake_config()`.

### Big Idea #2

`drake` is all built around *functions* rather than sourcing scripts.
- A plan works by using functions to create targets. 
- This allows `drake` to infer dependencies of objects and functions
- Running `drake_plan` creates a dataframe relating each target to the function used to generate it.


```{r, eval=FALSE}
bad_plan <- 
  drake_plan(
    first_target = source("import.R"),
    second_target = source("clean.R")
  )
```

Sourcing files breaks the dependency structure that makes `drake` useful. 

Instead, 

```{r, eval=FALSE}
source("all_my_funs.R")

good_plan <- 
  drake_plan(
    first_target = do_stuff(my_data),
    second_target = do_more_stuff(first_target)
  )
```

This setup allows `drake` to know `first_target` needs to be built before work on `second_target` can begin.

### Our plan

I'll illustrate a way you might want to use `drake` with something that's close to home for us.

Remember the [crazy blue light](https://twitter.com/NYCFireWire/status/1078478369036165121) from late December?

The Twitter account that let us know that this wasn't in fact aliens is [NYCFireWire](https://twitter.com/NYCFireWire).

Normally they just tweet out fires and their locations in a more or less predictable pattern, e.g.

https://twitter.com/NYCFireWire/status/1087888350277705728

What if we were constructing an analysis of these tweets and wanted to make sure our pipeline worked end-to-end, but didn't want to unnecessarily re-run outdated parts of it unless we needed to?


### The Pipeline

1. Pull in tweets, either the first big batch or any new ones that show up
2. Extract addresses from the tweets (`r emo::ji("notes")` regex time `r emo::ji("notes")`)
3. Send these to the Google Maps API to grab the addresses' latitudes and longitudes
4. Profit


### Grabbing tweets

- `get_seed_tweets` grabs a batch of tweets *or* reads in seed tweets from a file if the file exists
- `get_more_tweets` checks if there are new tweets and, if so, pulls in the right number of them
- `get_tweets` runs `get_seed_tweets` if given a null `tbl` argument, otherwise runs `get_more_tweets`

```{r, include=FALSE}
get_seed_tweets

get_more_tweets

get_tweets
```

```{r}
get_tweets()

old_tweet_id <- "1084619203167031297" # Random tweet from a while ago so what we can set a max id in the past

get_tweets(
  max_id = old_tweet_id
  ) %>% 
  get_tweets(n_tweets_reup = 5)
```


### Getting addresses

```{r, include=FALSE}
borough_reg

clean_borough

pull_addresses
```

```{r}
get_tweets() %>% 
  pull_addresses() %>% 
  select(street, borough, address, text)
```


Then we can use the [`geocode`](https://www.rdocumentation.org/packages/ggmap/versions/2.6.1/topics/geocode) function (along with a [Google Maps](https://cloud.google.com/maps-platform/) API key) from the `ggmap` package to attach the latitude and longitude to each address, if Google can find it. (Otherwise we're returned `NA`s.)

```{r}
geo_to_list

truncate_lat_long

get_lat_long
```

```{r}
get_tweets(n_tweets_seed = 5) %>% 
  pull_addresses() %>% 
  get_lat_long()
```


### Testing our reupping with a `r emo::ji("fire")` burner account `r emo::ji("fire")`

The `rtweet` package also supports posting tweets, so we can test out whether our trigger successfully pulls in new tweet by posting ourselves with a

[burner account!](https://twitter.com/didntstartit)

Every time `burner_plan` is run, new any tweets collected will be stored in `burner_path` and be read into the next iteration of `seed_burn`.

Let's define a file path to dump the tweets we collect into.

```{r}
burner_path <- "data/derived/burn.csv"

# Delete the file in burner_path if it exists already so we start from scratch
if (file.exists(burner_path)) fs::file_delete(burner_path)
```


Now we'll create our first draft of the plan:

```{r}
burner_plan <-
  drake_plan(
    # Reads in file from burner_path if file exists, otherwise pulls in seed tweets
    seed_burn = get_tweets(
      user = burner_handle,
      input = burner_path
    ),
    
    full_burn = get_tweets(
        tbl = seed_burn,
        user = burner_handle,
        output_path = burner_path # Outputs to burner_path
      )
  )
```

### Test out v1 of our plan

Since `seed_burn` hasn't changed and the code to generate the targets hasn't changed, every time after the first time we run this plan `drake` will let us know that our targets are up to date and not re-run anything.

```{r}
# Take a look at the dataframe that represents our plan
burner_plan 

# Save the config in an object we can look at
burner_config <- drake_config(burner_plan)

# Remove targets if they were already built
clean() 

# Both seed_burn and full_burn should be outdated
vis_drake_graph(burner_config) 

# Make the plan
make(burner_plan)  

# Everything should already be up to date 
outdated(burner_plan)

# Which is reflected in our graph
vis_drake_graph(burner_config) 
```

We can now load these targets into our working environment.

```{r}
# They won't be found if we don't loadd() them
seed_burn
loadd(seed_burn)
seed_burn
```


Every subsequent time we re-`make` the plan, `drake` should tell us we don't need to do anything.

```{r}
make(burner_plan)
make(burner_plan)
```

If we `clean()`, however, we'll remake the plan from scratch.

```{r}
clean()

# Targets are outdated now
vis_drake_graph(burner_config) 

# Make the plan again.
# Since we saved the previous output of full_burn to a file, this time we read that in for our seed_burn
make(burner_plan) 

# And now everything is up to date again
vis_drake_graph(burner_config) 
```


### Changing Code

One way we can guarantee that `drake` will re-`make` a target is if some part of the code used to generate that target changes.

Right now, nothing outdated becuase we just ran `make(burner_plan)`.

```{r}
outdated(burner_config)
vis_drake_graph(burner_config)
```

Let's modify the code to a function called by `get_tweets`. Note that `drake` recognizes that targets that incorporate this function at some point in the pipeline are out of date, even though this function isn't called in the plan directly.

Our original function, `there_are_new_tweets` which checks if the specified user has tweeted anything since the most recent tweet in our `tbl` argument:

```{r}
there_are_new_tweets <- function(tbl,
                                 user = firewire_handle,
                                 verbose = TRUE,
                                 ...) {
  latest_dt <-
    tbl %>%
    arrange(desc(created_at)) %>%
    slice(1) %>%
    pull(created_at)

  if (verbose) message("Searching for new tweets.")

  new <- get_seed_tweets(user = user, n_tweets = 1)

  if (max(new$created_at) <= latest_dt) {
    if (verbose) message("No new tweets to pull.")
    FALSE
  } else {
    TRUE
  }
}
```


We'll change the message to include a smiley emoji `emo::ji('smile')` when `there_are_new_tweets` is called.

```{r}
there_are_new_tweets <- function(tbl,
                                 user = firewire_handle,
                                 verbose = TRUE,
                                 ...) {
  latest_dt <-
    tbl %>%
    arrange(desc(created_at)) %>%
    slice(1) %>%
    pull(created_at)

  if (verbose) message(glue("Searching for new tweets! {emo::ji('smile')}"))

  new <- get_seed_tweets(user = user, n_tweets = 1)

  if (max(new$created_at) <= latest_dt) {
    if (verbose) message("No new tweets to pull.")
    FALSE
  } else {
    TRUE
  }
}
```


Now let's check that both targets which had been up to date has now been invalidated. 

```{r}
outdated(burner_config)
vis_drake_graph(burner_config)
```

Notice that we didn't even need to re-define our plan; `drake` can tell that a function a target in our plan relies on has meaningfully changed.

When we re-`make` our plan we should be messaged an `r emo::ji("smile")`.

```{r}
make(burner_plan)
```

And now `seed_burn` and `full_burn` should both be up to date again.

```{r}
outdated(burner_config)
vis_drake_graph(burner_config)
```



### Add Triggers

I mentioned that a way to guarantee that targets are re-made even if code *doesn't* change is to associate a trigger with a target.

Let's add a [trigger](https://ropensci.github.io/drake/articles/debug.html#test-with-triggers-) so that we always run `get_tweets` to look for new tweets at the burner handle.


```{r}
burner_plan_2 <-
  drake_plan(
    # Reads in file from burner_path if file exists, otherwise pulls in seed tweets
    seed_burn = get_tweets(
      user = burner_handle,
      input_path = burner_path
    ),
    
    full_burn = target(
      command = get_tweets(
        tbl = seed_burn,
        user = burner_handle,
        output_path = burner_path # Outputs to burner_path
      ),
      trigger = trigger(
        condition = TRUE # Always look for new tweets
      )
    )
  )

burner_plan_2
```


This trigger will make our `full_burn` target look always out of date to `drake` since it knows we need to re-make `full_burn` every time `make()` is run.

### Test out the `burner_plan` 2.0

First we'll clean out the previous file we saved and start from scratch.

```{r}
if (file.exists(burner_path)) fs::file_delete(burner_path)
```


Our `full_burn` target will always be out of date because the trigger indicates that it always needs to be rebuilt, but `seed_burn` will now be up-to-date.

```{r, eval=FALSE}
clean()

# Notice our new trigger column
burner_plan_2 
burner_config_2 <- drake_config(burner_plan_2)

# seed_burn is outdated becuase we haven't made the plan yet.
# full_burn is always out of date, no matter what, because of the trigger.
outdated(burner_config_2)
vis_drake_graph(burner_config_2)
```

All tweets are now stored in `burner_path`, which we specified as our `output_path` when making `full_burn`.

What's stored in this file is the same as both `seed_burn` and `full_burn` becuase there were no tweets posted between when we made `seed_burn` and `full_burn`.

```{r}
make(burner_plan_2)

# Load the results of seed_burn and full_burn into our environment
loadd(seed_burn) 
loadd(full_burn)
# These should be the same since no new tweets posted
expect_identical(seed_burn, full_burn) 

# full_burn always outdated, seed_burn no longer outdated
outdated(burner_config_2) 
vis_drake_graph(burner_config_2)
```


Now let's post a new tweet and re-`make` the plan.

`seed_burn` will be read in from the file, which reflects the state of the world before this tweet. Then `full_burn` will incorporate the new tweet.

```{r}
# Tweet here
post_tweet(status = 
             digest::digest(sample(100, 1)), 
           token = firewire_token)


# seed_burn (read from file) is up to date, and full_burn will always look outdated
make(burner_plan_2) 
vis_drake_graph(burner_config_2) 

loadd(seed_burn)
loadd(full_burn)
# We should have one extra row in full_burn than in seed_burn
expect_gt(nrow(full_burn), nrow(seed_burn)) 

seed_burn
full_burn

# Let's check that full_burn was saved to file
saved_burn <- read_csv(here("data", "derived", "burn.csv"))
expect_identical(nrow(full_burn), nrow(saved_burn))
```

Now we've proven that we can successfully trigger the re-building of `full_burn` every time and save it as the latest state of the world to a file. 

Every time new tweets arrive, that file will be updated at the end of the `make()` run.


### Write our full plan

Let's go back to our original NYCFireWire Twitter account.

For the purposes of illustration, I'll set a `max_id` on our `seed_fires` so that we can re-up and grab more tweets to build `fires`.

```{r}
fire_path <- here("data", "raw", "fires.csv")

plan <-
  drake_plan(
    seed_fires = get_tweets( # Grab some seed fires
      n_tweets_seed = 2,
      max_id = old_tweet_id
    ), 
    fires = target(
      command = get_tweets(
        tbl = seed_fires,
        n_tweets_reup = 3
      ),
      trigger = trigger(
        condition = TRUE # Always look for new tweets
      )
    ),
    addresses = pull_addresses(fires), # Extract addresses from tweets
    lat_long = get_lat_long(addresses), # Send to Google for lat-longs
    dat = join_on_city_data(lat_long, nyc), # Join on the nyc coords
    fire_sums = count_fires(dat), # Sum up n fires per lat-long combo

    time_graph = graph_fire_times(dat),
    plot = plot_fires(dat, nyc)
  )
```

### Run our plan

```{r}
plan

make(plan, verbose = 4)

# See what a couple of our targets look like
loadd(addresses)
addresses

loadd(dat)
dat
```

### Info drake stores

Let's get a little deeper into what `drake` stores in the `config`.

```{r, eval=FALSE}
config <- drake_config(plan)
sort(names(config))
config$plan
config$prework
config$targets
config$cache_path
config$graph

# The dependencies of a given target encompass both functions (get_tweets) and targets that it depends on (seed_fires)
deps_target(fires)

# Since we have a trigger on fires, everything downstream of fires is also perpetually outdated
outdated(config)
vis_drake_graph(config)

# We can clean just one target instead of cleaning everything
clean(fires)

# Since we didn't clean() seed_fires, this target remains built and our dependency graph looks the same
vis_drake_graph(config)

# And again, if we re-make the plan we'll pull in new tweets because of our trigger
make(plan)
```

### Previously `geocode`d

For kicks, let's make our plots with data from 3000 tweets pulled from NYCFireWire and sent to Google for geocoding. 

We'll stop at `pull_addresses(fires)` so we don't re-geocode all 3k fires.

We'll set the `input_path` to `seed_fires` to that file, and still grab the most recent 3 tweets.

```{r}
big_plan <-
  drake_plan(
    seed_fires = get_tweets( # Grab seed fires from file
      input_path = here("data", "derived", "lots_o_fires.csv")
    ), 
    fires = target(
      command = get_tweets(
        tbl = seed_fires,
        n_tweets_reup = 3
      ),
      trigger = trigger(
        condition = TRUE 
      )
    ),
    addresses = pull_addresses(fires)
    # Not sending all 3k addresses to Google
  )

make(big_plan)

loadd(addresses)
addresses
```

Make our final plot from the 3k fires:

```{r}
dat <- 
  read_csv(here("data", "derived", "dat.csv"))

fire_sums <-
  read_csv(here("data", "derived", "fire_sums.csv"))

graph_fire_times(dat)
plot_fire_sums(fire_sums)
```


### Moar Resources

- [`drake` user manual](https://ropenscilabs.github.io/drake-manual/index.html)
- [debugging drake](https://ropensci.github.io/drake/articles/debug.html)
- [Sina Rüeger's `drake` presentation](https://sinarueeger.github.io/2018/10/09/workflow/)
- [Kirill Müller's cheat sheet](https://github.com/krlmlr/drake-sib-zurich/blob/master/cheat-sheet.pdf)

---
title: "pres"
author: "Amanda Dobbyn"
date: "1/20/2019"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source(here::here("didnt_start_it.R"))
```

### Quick about me

Data Scientist at Earlybird Software, former co-organzier of R-Ladies Chicago

Ultimate firsbee player

### `drake`'s main idea

In a pipeline, when changes occur that make the most recent results out-of-date, rebuild *only* the parts of the pipeline that need to be rebuilt.

### What is `drake`?

A workflow manager for your R code 

[GNU Make](https://www.gnu.org/software/make/) for R, built around dataframes.

GNU Make uses workflow outline file called a Makefile to specify dependencies and how targets should be reubilt when they're out of date. 

Created and maintained by [Will](https://twitter.com/wmlandau) [Landau](https://github.com/wlandau) and friends.

### Two pieces of vocab

*targets* and *commands*
From `?drake_config`: "**Targets** are the objects and files that drake generates, and **commands** are the pieces of R code that produce them."

### Analogy to `knitr`

`drake` is sort of like `knitr` in the sense that 
1) It's reproducible and compact. You expect to be able to rerun someone's report from a single file.

2) In `knitr`, chunks can be cached if they've already been run, so they don't need to be re-run unless something in them changes.

3) When an Rmd is knit, a chunk successfully knitting depends on the previous chunk knitting and on any chunk that you specify a `depedson` for.


### Other nice features of `drake`
1. See how your pipeline fits together, in a tidy dataframe
2. Visualize dependencies in your code in graph format
3. Some high-performance computing advantages
4. Great for iteration and reproducibility; you know exactly how these results were generated
5. It's all in R -- no writing config files!

### How to `drake`:
1. Store prework (loading packages and user-defined functions) in a file 
2. Store a `drake` plan in another file
3. Create the plan and run it

```{r, eval=FALSE}
source("/path/to/my/prework.R")

# Custom functions defined in prework are:
# clean_my(), analyze_my(), and report_out_my()

plan <- 
  drake_plan(
  cleaned_data = clean_my(raw_data),
  results = analyze_my(cleaned_data),
  report = report_out_my(results,
                         file_out = "/path/to/my/report.md")
)

make(plan)
```

The first run of `make(plan)` will run the plan from scratch. 

After that, `drake` will only re-run the parts of the plan that are out of date and everything downstream of that.

`drake` knows that, for example, `results` depends on `cleaned_data` because `cleaned_data` is part of the `analyze_my(cleaned_data)` command used to generate `results`.


### What makes a target become out of date?

1. Some part of the code used to generate that target or one of its upstream targets has changed
2. A trigger is activated (more on that later)


### Where is all this stored?

How does `drake` store info/data about targets?
In a hidden `.drake` cache when you run `make()`. In the `.drake/data` subdirectory, targets are stored as hashed `rds` objects. [More on storage.](https://ropensci.github.io/drake/articles/storage.html)

`clean()` cleans that cache.

How does `drake` keep track of dependencies? 
`drake` stores dependency graph along with a bunch of other things in `config`, which you can access with `drake_config()`.

### Big Idea #2

`drake` is all built around *functions* rather than sourcing scripts.
- A plan works by using functions to create targets. 
- This allows `drake` to infer dependencies of objects and functions
- Running `drake_plan` creates a dataframe relating each target to the function used to generate it.


```{r, eval=FALSE}

bad_plan <- 
  drake_plan(
    first_target = source("import.R"),
    second_target = source("clean.R")
  )
```

Sourcing files breaks the dependency structure that `drake` is all about. 


Instead, 

```{r, eval=FALSE}

source("all_my_funs.R")

good_plan <- 
  drake_plan(
    first_target = do_stuff(dat),
    second_target = do_more_stuff(first_target)
  )
```

This setup allows `drake` to know `first_target` needs to be built before `second_target` can begin.

### Our plan

Remember the [crazy blue light](https://twitter.com/NYCFireWire/status/1078478369036165121) from late December?

The Twitter account that let us know that this wasn't in fact aliens is [NYCFireWire](https://twitter.com/NYCFireWire).

Normally they just tweet out fires and their locations in a more or less predictable pattern, e.g.

https://twitter.com/NYCFireWire/status/1087888350277705728


What if we were constructing an analysis of these tweets and wanted to make sure our pipeline worked end-to-end, but didn't want to unnecessarily re-run outdated parts of it unless we needed to?


### The Pipeline

1. Pull in tweets, either the first big batch or any new ones that show up
2. Extract addresses from the tweets (`r emo::ji("notes")` regex time `r emo::ji("notes")`)
3. Send these to the Google Maps API to grab the addresses' latitudes and longitudes
4. Profit


### Grabbing tweets

- `get_seed_tweets` grabs a batch of tweets *or* reads in seed tweets from a file if the file exists
- `get_more_tweets` checks if there are new tweets and, if so, pulls in the right number of them
- `get_tweets` runs `get_seed_tweets` if given a null `tbl` argument, otherwise runs `get_more_tweets`

```{r, include=FALSE}
get_seed_tweets

get_more_tweets

get_tweets
```

```{r}
get_tweets()

old_tweet_id <- "1084619203167031297" # Random tweet from a while ago so what we can set a max id in the past

get_tweets(
  max_id = old_tweet_id
  ) %>% 
  get_tweets(n_tweets_reup = 5)
```


### Getting addresses

```{r, include=FALSE}
borough_reg

clean_borough

pull_addresses
```

```{r}
get_tweets() %>% 
  pull_addresses() %>% 
  select(street, borough, address, text)
```


Then we can use the `geocode` function (along with a Google Maps API key) from the `ggmap` package to attach the latitude and longitude to each address, if Google can find it. (Otherwise we're returned `NA`s.)

```{r}
geo_to_list

truncate_lat_long

get_lat_long
```

```{r}
get_tweets(n_tweets_seed = 5) %>% 
  pull_addresses() %>% 
  get_lat_long()
```


### Testing our reupping with a `r emo::ji("fire")` burner account `r emo::ji("fire")`

The `rtweet` package also supports posting tweets, so we can test out whether our trigger successfully pulls in new tweet by posting ourselves.

[Burner account!](https://twitter.com/didntstartit)

Every time `burner_plan` is run, new any tweets collected will be stored in `burner_path` and be read into the next iteration of `seed_burn`.

Let's pre-store all tweets in burner_path.

```{r}
burner_path <- "data/derived/burn.csv"

get_tweets(
      user = burner_handle,
      output_path = burner_path
    )

read_csv(burner_path)
```


Now we'll create our first draft of the plan:

```{r}
burner_plan <-
  drake_plan(
    # Reads in file from burner_path if file exists, otherwise pulls in seed tweets
    seed_burn = get_tweets(
      user = burner_handle
    ),
    
    full_burn = get_tweets(
        tbl = seed_burn,
        user = burner_handle,
        output_path = burner_path # Outputs to burner_path
      )
  )
```

### Test out v1 of our plan

Since `seed_burn` hasn't changed and the code to generate the targets hasn't changed, every time after the first time we run this plan `drake` will let us know that our targets are up to date and not re-run anything.

```{r}
burner_plan
burner_config <- drake_config(burner_plan)
vis_drake_graph(burner_config)
make(burner_plan)  # Everything should already be up to date 
vis_drake_graph(burner_config) # Which is reflected in our graph
make(burner_plan)
```


### Add Triggers

Now let's add a [trigger](https://ropensci.github.io/drake/articles/debug.html#test-with-triggers-) so that we always trigger `get_tweets` to look for new tweets at the burner handle.

Notice that we now have a new column in our dataframe for `trigger`, which `drake_plan` has automatically added for us.

This trigger will make our `full_burn` target look always out of date to `drake` since it knows we need to re-make `full_burn` every time `make()` is run.

```{r}
burner_plan_2 <-
  drake_plan(
    # Reads in file from burner_path if file exists, otherwise pulls in seed tweets
    seed_burn = get_tweets(
      user = burner_handle,
      input_path = burner_path
    ),
    
    full_burn = target(
      command = get_tweets(
        tbl = seed_burn,
        user = burner_handle,
        output_path = burner_path # Outputs to burner_path
      ),
      trigger = trigger(
        condition = TRUE # Always look for new tweets
      )
    )
  )
```

### Test out the `burner_plan` 2.0

The data in `burner_path` should be all the tweets this burner account has tweeted so far:

```{r, eval=FALSE}
clean()
burner_plan_2
burner_config_2 <- drake_config(burner_plan_2)
# Our full_burn target will always be out of date because the trigger means it always needs to be rebuilt
vis_drake_graph(burner_config_2)

make(burner_plan_2)
loadd(seed_burn)
loadd(full_burn)
expect_equal(seed_burn, full_burn)

outdated(burner_config_2)
vis_drake_graph(burner_config_2)
```

Now let's post a new tweet.

```{r}
# Tweet here
post_tweet(status = 
             digest::digest(sample(100, 1)), 
           token = firewire_token)

make(burner_plan_2)
vis_drake_graph(burner_config_2)

loadd(seed_burn)
loadd(full_burn)

seed_burn
full_burn

# And let's checked it was saved to file
saved_burn <- read_csv(here("data", "derived", "burn.csv"))
expect_gt(nrow(full_burn), nrow(seed_burn))
expect_equal(nrow(full_burn), nrow(saved_burn))

outdated(burner_config_2)

make(burner_plan_2)
loadd(seed_burn)
loadd(full_burn)
expect_equal(nrow(seed_burn), nrow(full_burn))
```


### Write our full plan

```{r}
fire_path <- here("data", "raw", "fires.csv")

plan <-
  drake_plan(
    seed_fires = get_tweets( # Grab some seed fires
      n_tweets_seed = 2,
      max_id = old_tweet_id
    ), 
    fires = target(
      command = get_tweets(
        tbl = seed_fires,
        n_tweets_reup = 3
      ),
      trigger = trigger(
        condition = TRUE # Always look for new tweets
      )
    ),
    addresses = pull_addresses(fires), # Extract addresses from tweets
    lat_long = get_lat_long(addresses), # Send to Google for lat-longs
    dat = join_on_city_data(lat_long, nyc), # Join on the nyc coords
    fire_sums = count_fires(dat), # Sum up n fires per lat-long combo

    time_graph = graph_fire_times(dat),
    plot = plot_fires(dat, nyc)
  )
```

### Run our plan

```{r}
plan

make(plan, verbose = 4)
```

### Info drake stores

```{r, eval=FALSE}
config <- drake_config(plan)
sort(names(config))
config$plan
config$prework
config$targets
config$cache_path
config$graph

deps_target(fires)

vis_drake_graph()

lat_long
loadd(lat_long)
lat_long

clean(fires)

outdated(config)

vis_drake_graph()

make(plan)

loadd(lat_long)
lat_long
```

If our `seed_fires` had been 3000 instead of 3:

```{r}
big_plan <-
  drake_plan(
    seed_fires = get_tweets( # Grab seed fires from file
      input_path = here("data", "derived", "lots_o_fires.csv")
    ), 
    fires = target(
      command = get_tweets(
        tbl = seed_fires,
        n_tweets_reup = 3
      ),
      trigger = trigger(
        condition = TRUE 
      )
    ),
    addresses = pull_addresses(fires)
    # Not sending all 3k addresses to Google
  )

make(big_plan)

loadd(addresses)
addresses

make(big_plan)
```

Make our final plot:

```{r}
dat <- 
  read_csv(here("data", "derived", "dat.csv"))

fire_sums <-
  read_csv(here("data", "derived", "fire_sums.csv"))

plot_fire_sums(fire_sums)
```


### Moar Resources

- [`drake` user manual](https://ropenscilabs.github.io/drake-manual/index.html)
- [debugging drake](https://ropensci.github.io/drake/articles/debug.html)
- [Sina Rüeger's `drake` presentation](https://sinarueeger.github.io/2018/10/09/workflow/)
- [Kirill Müller's cheat sheet](https://github.com/krlmlr/drake-sib-zurich/blob/master/cheat-sheet.pdf)
